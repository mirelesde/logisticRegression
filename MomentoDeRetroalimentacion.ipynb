{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cargando datos\n",
        "\n",
        "Cargamos los datos que utilizaremos. Utillizaremos el clásico wine dataset, un dataset para clasificación que tiene tres clases. Los tres vinos son italianos, las clases representan de qué cultivo son (no tienen nombre, solo 1, 2 o 3). En este caso solo queremos predecir si cada caso corrsponde con la clase 2 o no."
      ],
      "metadata": {
        "id": "avdJoM-n5WcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = load_wine()\n",
        "X = wine[\"data\"]\n",
        "y = (wine[\"target\"] == 2).astype(int) # Si el vino es\n"
      ],
      "metadata": {
        "id": "baCeFucA4KA2"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escalamiento"
      ],
      "metadata": {
        "id": "-AxKd7Ls5sf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escalamos los features para acelerar la convergencia y mejorar el performance del modelo"
      ],
      "metadata": {
        "id": "O8xc8D1V_EFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "metadata": {
        "id": "oQ1OMUKf6Idp"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dividiendo los datos en test y train"
      ],
      "metadata": {
        "id": "x7b4RCq2z2VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos la función *test_train_split* para dividir nuestros datos en un conjunto de entrenamiento y otro de prueba."
      ],
      "metadata": {
        "id": "UeY0poFJ_FDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvrME8Zjz7nj",
        "outputId": "6fff4897-dd4c-47b0-ff3c-e00a5ca0e18a"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142, 13)\n",
            "(36, 13)\n",
            "(142,)\n",
            "(36,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creando Theta"
      ],
      "metadata": {
        "id": "MOx49RGR0A7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para theta, tenemos un theta para cada feature. También añadimos uno final que es un bias."
      ],
      "metadata": {
        "id": "voqV9_JU_cv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "theta = np.random.randn(len(X_train[0]) + 1, 1)\n",
        "theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icdL1Wwl6bE1",
        "outputId": "c2452db7-9f01-403f-867a-2f65acbfd207"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.0572776 ],\n",
              "       [ 0.84188192],\n",
              "       [ 0.06329814],\n",
              "       [ 1.70986935],\n",
              "       [ 0.25147447],\n",
              "       [-0.39390336],\n",
              "       [ 0.63540096],\n",
              "       [ 1.13264269],\n",
              "       [ 0.38746993],\n",
              "       [ 0.89410252],\n",
              "       [-0.06788595],\n",
              "       [-0.31127277],\n",
              "       [ 1.05581012],\n",
              "       [ 0.13599554]])"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Añadimos un uno al vector de features para que el bias que añadimos anteriormente si sea considerado al multiplicar."
      ],
      "metadata": {
        "id": "YVg22WHH_pow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_vect = np.c_[np.ones((len(X_train), 1)), X_train]\n",
        "print(X_vect[:5])\n",
        "print(X_vect.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_82SJpA7C3a",
        "outputId": "f59c3f63-5d86-4194-99e2-b42f190e11e5"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.         -0.82839089 -1.20856726 -1.5225111  -1.40982117  2.54582528\n",
            "  -0.633101   -0.17998078 -0.09551669  2.04836353 -0.71723991  0.44992405\n",
            "  -0.42611337  0.0098934 ]\n",
            " [ 1.          0.36981612 -0.55327317 -0.82799632 -0.74919419 -0.40313502\n",
            "   0.16806478  0.16136791 -0.74014117 -0.4220751  -0.47932642  0.2744305\n",
            "   0.22361033  1.71359755]\n",
            " [ 1.          0.13511578 -0.3916938   1.40176163  1.80322825  1.14155847\n",
            "  -0.15240153 -0.75224181 -0.82071924 -0.05413743  0.88326902 -1.52437837\n",
            "  -1.81030733 -1.02506707]\n",
            " [ 1.         -0.18605311 -0.66099274  0.56103322 -0.50896619 -0.33292168\n",
            "   0.2962513   0.34208192 -0.82071924 -0.22934584 -0.48797782  0.58154421\n",
            "   1.43831115  0.85378424]\n",
            " [ 1.          1.39508604  1.58316512  1.36520822  1.50294326 -0.26270834\n",
            "  -0.39275127 -1.2743045   1.59662258 -0.4220751   1.79166599 -1.52437837\n",
            "  -1.42894777 -0.59516041]]\n",
            "(142, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def sigmoid_function(X):\n",
        "  return 1/(1+math.e**(-X)) # esta es la forma general de un modelo logístico\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def log_regression(X, y, theta, alpha, epochs):\n",
        "  y_ = np.reshape(y, (len(y), 1)) # para que shape sea (len(y),1)\n",
        "  N = len(X)\n",
        "  avg_loss_list = []\n",
        "  for epoch in range(epochs):\n",
        "    sigmoid_x_theta = sigmoid_function(X_vect.dot(theta))\n",
        "    grad = (1/N) * X_vect.T.dot(sigmoid_x_theta - y_)\n",
        "    theta = theta - (alpha * grad)\n",
        "    hyp = sigmoid_function(X_vect.dot(theta))\n",
        "    avg_loss = -np.sum(np.dot(y_.T, np.log(hyp) + np.dot((1-y_).T, np.log(1-hyp)))) / len(hyp)\n",
        "    if epoch % 10000 == 0:\n",
        "      print('epoch: {} | avg_loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "    avg_loss_list.append(avg_loss)\n",
        "  plt.plot(np.arange(1, epochs), avg_loss_list[1:], color='red')\n",
        "  plt.title('Cost function')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Cost')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "aQ_GrKJ17KaN"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000000\n",
        "log_regression(X_train, y_train, theta, alpha, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IizQbC2a7O1g",
        "outputId": "1c6126db-4c3a-4f64-85cd-250708dc0ca3"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 | avg_loss: 38.74754423581463\n",
            "epoch: 10000 | avg_loss: 0.009527184743469181\n",
            "epoch: 20000 | avg_loss: 0.004947974062234787\n",
            "epoch: 30000 | avg_loss: 0.003359879920971949\n",
            "epoch: 40000 | avg_loss: 0.002549320909057094\n",
            "epoch: 50000 | avg_loss: 0.002056372187895425\n",
            "epoch: 60000 | avg_loss: 0.001724483873172596\n",
            "epoch: 70000 | avg_loss: 0.0014855983459254596\n",
            "epoch: 80000 | avg_loss: 0.0013053199740277054\n",
            "epoch: 90000 | avg_loss: 0.0011643767588486523\n",
            "epoch: 100000 | avg_loss: 0.0010511242000886395\n",
            "epoch: 110000 | avg_loss: 0.0009581081535143468\n",
            "epoch: 120000 | avg_loss: 0.0008803337607873532\n",
            "epoch: 130000 | avg_loss: 0.0008143275577253895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-173-3248a1e15091>:16: RuntimeWarning: divide by zero encountered in log\n",
            "  avg_loss = -np.sum(np.dot(y_.T, np.log(hyp) + np.dot((1-y_).T, np.log(1-hyp)))) / len(hyp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 140000 | avg_loss: nan\n",
            "epoch: 150000 | avg_loss: nan\n",
            "epoch: 160000 | avg_loss: nan\n",
            "epoch: 170000 | avg_loss: nan\n",
            "epoch: 180000 | avg_loss: nan\n",
            "epoch: 190000 | avg_loss: nan\n",
            "epoch: 200000 | avg_loss: nan\n",
            "epoch: 210000 | avg_loss: nan\n",
            "epoch: 220000 | avg_loss: nan\n",
            "epoch: 230000 | avg_loss: nan\n",
            "epoch: 240000 | avg_loss: nan\n",
            "epoch: 250000 | avg_loss: nan\n",
            "epoch: 260000 | avg_loss: nan\n",
            "epoch: 270000 | avg_loss: nan\n",
            "epoch: 280000 | avg_loss: nan\n",
            "epoch: 290000 | avg_loss: nan\n",
            "epoch: 300000 | avg_loss: nan\n",
            "epoch: 310000 | avg_loss: nan\n",
            "epoch: 320000 | avg_loss: nan\n",
            "epoch: 330000 | avg_loss: nan\n",
            "epoch: 340000 | avg_loss: nan\n",
            "epoch: 350000 | avg_loss: nan\n",
            "epoch: 360000 | avg_loss: nan\n",
            "epoch: 370000 | avg_loss: nan\n",
            "epoch: 380000 | avg_loss: nan\n",
            "epoch: 390000 | avg_loss: nan\n",
            "epoch: 400000 | avg_loss: nan\n",
            "epoch: 410000 | avg_loss: nan\n",
            "epoch: 420000 | avg_loss: nan\n",
            "epoch: 430000 | avg_loss: nan\n",
            "epoch: 440000 | avg_loss: nan\n",
            "epoch: 450000 | avg_loss: nan\n",
            "epoch: 460000 | avg_loss: nan\n",
            "epoch: 470000 | avg_loss: nan\n",
            "epoch: 480000 | avg_loss: nan\n",
            "epoch: 490000 | avg_loss: nan\n",
            "epoch: 500000 | avg_loss: nan\n",
            "epoch: 510000 | avg_loss: nan\n",
            "epoch: 520000 | avg_loss: nan\n",
            "epoch: 530000 | avg_loss: nan\n",
            "epoch: 540000 | avg_loss: nan\n",
            "epoch: 550000 | avg_loss: nan\n",
            "epoch: 560000 | avg_loss: nan\n",
            "epoch: 570000 | avg_loss: nan\n",
            "epoch: 580000 | avg_loss: nan\n",
            "epoch: 590000 | avg_loss: nan\n",
            "epoch: 600000 | avg_loss: nan\n",
            "epoch: 610000 | avg_loss: nan\n",
            "epoch: 620000 | avg_loss: nan\n",
            "epoch: 630000 | avg_loss: nan\n",
            "epoch: 640000 | avg_loss: nan\n",
            "epoch: 650000 | avg_loss: nan\n",
            "epoch: 660000 | avg_loss: nan\n",
            "epoch: 670000 | avg_loss: nan\n",
            "epoch: 680000 | avg_loss: nan\n",
            "epoch: 690000 | avg_loss: nan\n",
            "epoch: 700000 | avg_loss: nan\n",
            "epoch: 710000 | avg_loss: nan\n",
            "epoch: 720000 | avg_loss: nan\n",
            "epoch: 730000 | avg_loss: nan\n",
            "epoch: 740000 | avg_loss: nan\n",
            "epoch: 750000 | avg_loss: nan\n",
            "epoch: 760000 | avg_loss: nan\n",
            "epoch: 770000 | avg_loss: nan\n",
            "epoch: 780000 | avg_loss: nan\n",
            "epoch: 790000 | avg_loss: nan\n",
            "epoch: 800000 | avg_loss: nan\n",
            "epoch: 810000 | avg_loss: nan\n",
            "epoch: 820000 | avg_loss: nan\n",
            "epoch: 830000 | avg_loss: nan\n",
            "epoch: 840000 | avg_loss: nan\n",
            "epoch: 850000 | avg_loss: nan\n",
            "epoch: 860000 | avg_loss: nan\n",
            "epoch: 870000 | avg_loss: nan\n",
            "epoch: 880000 | avg_loss: nan\n",
            "epoch: 890000 | avg_loss: nan\n",
            "epoch: 900000 | avg_loss: nan\n",
            "epoch: 910000 | avg_loss: nan\n",
            "epoch: 920000 | avg_loss: nan\n",
            "epoch: 930000 | avg_loss: nan\n",
            "epoch: 940000 | avg_loss: nan\n",
            "epoch: 950000 | avg_loss: nan\n",
            "epoch: 960000 | avg_loss: nan\n",
            "epoch: 970000 | avg_loss: nan\n",
            "epoch: 980000 | avg_loss: nan\n",
            "epoch: 990000 | avg_loss: nan\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probando el modelo con los datos de preuba"
      ],
      "metadata": {
        "id": "AZtYMFyZ02jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.c_[np.ones((len(X_test), 1)), X_test]\n",
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI0RN8dt2UDp",
        "outputId": "b337bb76-d26a-48ee-e066-e55630811f77"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.00000000e+00 -1.96483466e+00 -1.43298305e+00  4.87926405e-01\n",
            "   4.51945783e-01 -8.24415067e-01  2.96251299e-01 -1.93461003e-02\n",
            "   4.68529735e-01 -2.64387527e-01 -8.55662307e-01  6.25417599e-01\n",
            "  -4.26113366e-01 -9.96406622e-01]\n",
            " [ 1.00000000e+00 -1.70542902e+00 -3.10904118e-01 -3.16248596e-01\n",
            "  -4.48909194e-01 -1.22281661e-01  1.16151034e+00  2.31645580e-01\n",
            "  -1.54592178e+00 -4.22075098e-01 -7.82125409e-01  8.88657922e-01\n",
            "   4.91974464e-01 -1.27982657e+00]\n",
            " [ 1.00000000e+00  1.35115780e-01 -1.19061400e+00 -2.43634632e+00\n",
            "  -1.34976417e+00 -1.52654847e+00  1.09741707e+00  1.15529496e+00\n",
            "  -8.20719236e-01  1.20736314e+00  1.04643056e-01  7.13164373e-01\n",
            "   8.02711883e-01 -7.79861048e-01]\n",
            " [ 1.00000000e+00 -1.13720713e+00 -9.03361794e-01 -2.43141777e-01\n",
            "   1.23268676e+00 -2.08825520e+00 -1.52401534e-01 -4.41012124e-01\n",
            "   4.68529735e-01 -3.69512574e-01 -1.43530608e+00  4.93797437e-01\n",
            "   8.45085168e-01 -3.88168318e-01]\n",
            " [ 1.00000000e+00  8.26864161e-01 -9.75174845e-01 -1.63217132e+00\n",
            "  -4.48909194e-01 -4.03135023e-01 -3.12634689e-01 -2.40218779e-01\n",
            "  -3.37250872e-01 -1.50836726e+00 -5.44211919e-01  1.19577163e+00\n",
            "  -2.14246944e-01 -3.72245850e-01]\n",
            " [ 1.00000000e+00  4.93342620e-01  2.03199669e+00  1.80384913e+00\n",
            "   1.65308575e+00  8.60705108e-01 -5.04914475e-01 -1.07351116e+00\n",
            "  -7.40141175e-01 -8.42575290e-01  1.48886700e+00 -1.26113805e+00\n",
            "  -9.76966064e-01 -3.72245850e-01]\n",
            " [ 1.00000000e+00 -1.86053111e-01  8.38104709e-01  7.80353678e-01\n",
            "   7.52230776e-01  4.39425064e-01 -1.03368389e+00 -1.43493918e+00\n",
            "   1.91893483e+00 -1.10538791e+00  2.25762651e-01 -3.83670306e-01\n",
            "  -7.08601929e-01 -5.63315474e-01]\n",
            " [ 1.00000000e+00  6.09998823e-02  3.10919247e+00 -8.64549732e-01\n",
            "   6.02088279e-01 -9.64841748e-01  5.20577716e-01  6.23192602e-01\n",
            "  -4.98406993e-01  7.34300428e-01 -1.06329590e+00 -9.97897726e-01\n",
            "   6.89716458e-01 -1.16836928e+00]\n",
            " [ 1.00000000e+00 -1.23602833e+00  9.81730812e-01 -1.33974405e+00\n",
            "  -1.48624201e-01 -8.94628408e-01 -4.72867844e-01 -3.90813788e-01\n",
            "   6.56394314e-02  4.89008649e-01 -1.63428828e+00 -1.20429983e-01\n",
            "   6.19094317e-01 -5.82422436e-01]\n",
            " [ 1.00000000e+00  1.01215391e+00 -5.26343273e-01  1.95499132e-01\n",
            "  -1.65004916e+00  7.90491768e-01  2.53951547e+00  1.71751633e+00\n",
            "  -3.37250872e-01  4.89008649e-01  8.61640526e-01  2.30557114e-01\n",
            "   9.15707308e-01  1.41107064e+00]\n",
            " [ 1.00000000e+00  1.70390229e+00  1.12535692e+00 -3.16248596e-01\n",
            "  -1.04947918e+00  1.58571702e-01  1.53004659e+00  1.14525530e+00\n",
            "  -7.40141175e-01  1.04967557e+00 -6.83849376e-02  3.62177276e-01\n",
            "   1.16994702e+00  1.01300893e+00]\n",
            " [ 1.00000000e+00 -7.78980294e-01 -1.25345042e+00 -3.67916223e+00\n",
            "  -2.67101814e+00 -8.24415067e-01 -5.04914475e-01 -1.46505818e+00\n",
            "  -6.59563114e-01 -2.05151334e+00 -1.34446639e+00  4.06050663e-01\n",
            "  -1.11821035e+00 -7.22540161e-01]\n",
            " [ 1.00000000e+00 -7.78980294e-01 -1.08289442e+00 -7.54889505e-01\n",
            "  -1.48624201e-01 -8.94628408e-01  1.93062948e+00  1.07497763e+00\n",
            "  -1.38476566e+00  4.89008649e-01 -2.63041430e-01  1.15189824e+00\n",
            "   3.64854610e-01 -1.04098953e+00]\n",
            " [ 1.00000000e+00 -7.17217046e-01 -6.52016113e-01 -6.45229278e-01\n",
            "   9.02373272e-01  5.79851746e-01 -4.72867844e-01  6.09712375e-02\n",
            "  -1.76094750e-01  3.34667755e-02 -1.29688369e+00  4.49924050e-01\n",
            "   4.91974464e-01 -1.27982657e+00]\n",
            " [ 1.00000000e+00  6.04516467e-01 -6.07132956e-01 -4.62462232e-01\n",
            "   1.35280076e+00 -8.94628408e-01 -6.65147630e-01 -1.90020443e-01\n",
            "  -7.40141175e-01 -9.82742020e-01 -5.70166118e-01  9.89369528e-02\n",
            "   2.37734757e-01 -8.75395860e-01]\n",
            " [ 1.00000000e+00  1.08626980e+00  2.42696848e+00 -4.99015641e-01\n",
            "   1.51660791e-01 -1.38612179e+00 -2.10724602e+00 -1.69597053e+00\n",
            "   3.07373613e-01 -1.59597147e+00 -6.83849376e-02 -1.65599853e+00\n",
            "  -1.81030733e+00 -1.05691200e+00]\n",
            " [ 1.00000000e+00  1.46920194e+00 -6.69969376e-01  4.14819587e-01\n",
            "  -8.99336682e-01  5.79851746e-01  1.61016317e+00  1.90827001e+00\n",
            "  -3.37250872e-01  4.71487808e-01  1.57538100e+00  1.19577163e+00\n",
            "   2.94232470e-01  2.97147258e+00]\n",
            " [ 1.00000000e+00  4.80989970e-01 -5.08390010e-01  9.26567314e-01\n",
            "  -1.01945068e+00 -4.73348364e-01  8.89113972e-01  9.14342951e-01\n",
            "  -1.76094750e-01 -2.46866685e-01 -1.11641936e-01 -1.64303370e-01\n",
            "   8.59209596e-01  1.42699311e+00]\n",
            " [ 1.00000000e+00  1.65449169e+00 -5.89179693e-01  1.21899459e+00\n",
            "   1.65308575e+00 -1.22281661e-01  8.08997395e-01 -7.22122806e-01\n",
            "   1.35488840e+00  1.94323848e+00  3.43543192e+00 -1.69987192e+00\n",
            "  -9.20468352e-01 -2.76711037e-01]\n",
            " [ 1.00000000e+00 -4.45458753e-01 -8.76431899e-01 -1.26663723e+00\n",
            "  -8.09251184e-01  1.81450206e-02 -4.40821213e-01 -6.21726134e-01\n",
            "   1.35488840e+00 -1.70109651e+00  2.99299548e-01  9.89369528e-02\n",
            "  -1.44307219e+00 -9.45454722e-01]\n",
            " [ 1.00000000e+00 -9.64270039e-01 -9.39268319e-01 -1.55906451e+00\n",
            "  -1.48624201e-01 -5.43561704e-01  1.03971513e-01  1.07729013e-02\n",
            "   2.26795553e-01  8.56946317e-01 -1.02003890e+00 -4.27543693e-01\n",
            "   5.76721033e-01 -1.38491486e+00]\n",
            " [ 1.00000000e+00 -1.45837602e+00 -5.53273167e-01 -1.77838496e+00\n",
            "   1.51829490e-03 -9.64841748e-01  3.28297930e-01 -3.90813788e-01\n",
            "   6.56394314e-02 -2.99429209e-01 -1.29688369e+00 -7.65565958e-02\n",
            "  -2.42495800e-01 -1.05691200e+00]\n",
            " [ 1.00000000e+00  1.30861750e+00 -1.67278014e-01  8.90013905e-01\n",
            "  -5.69023190e-01  1.49262517e+00  4.88531085e-01  4.82637261e-01\n",
            "  -4.17828932e-01 -5.97283511e-01 -3.49944013e-03  4.49924050e-01\n",
            "   1.36768901e+00  1.74544249e+00]\n",
            " [ 1.00000000e+00  4.19226722e-01 -1.25345042e+00 -2.38213228e-02\n",
            "  -7.49194186e-01  7.20278427e-01  3.76367877e-01 -7.32162473e-01\n",
            "   1.51604452e+00 -2.05151334e+00 -8.16731008e-01  2.74430501e-01\n",
            "  -9.62841636e-01  9.89339909e-03]\n",
            " [ 1.00000000e+00 -1.49543397e+00 -1.85231277e-01  1.51142186e+00\n",
            "   2.70408323e+00 -5.43561704e-01 -2.64564743e-01  2.11566246e-01\n",
            "   1.75777870e+00  2.96279395e-01 -8.90267905e-01  5.50635657e-02\n",
            "  -2.42495800e-01 -8.94502823e-01]\n",
            " [ 1.00000000e+00  1.50625989e+00 -5.71226430e-01 -2.43141777e-01\n",
            "  -9.59393680e-01  1.28198515e+00  1.44993001e+00  9.74580955e-01\n",
            "  -8.20719236e-01  7.69342110e-01  5.71818637e-01 -7.65565958e-02\n",
            "   9.86329449e-01  7.10482022e-01]\n",
            " [ 1.00000000e+00 -7.17217046e-01  1.87939396e+00  1.32865481e+00\n",
            "   2.10351324e+00  1.58571702e-01 -1.52401534e-01  1.01129906e-01\n",
            "   5.49107795e-01  2.08675189e-01 -1.28823229e+00 -1.64303370e-01\n",
            "   7.17965314e-01 -1.21613669e+00]\n",
            " [ 1.00000000e+00  2.95700226e-01  1.47544554e+00 -2.79695187e-01\n",
            "  -5.99051690e-01  2.28785042e-01  5.52624347e-01  6.03113268e-01\n",
            "  -3.37250872e-01  1.21070982e-01 -3.01972728e-01 -6.03037242e-01\n",
            "   5.48472176e-01 -2.13021163e-01]\n",
            " [ 1.00000000e+00  7.15690314e-01 -6.07132956e-01 -2.38213228e-02\n",
            "  -1.18595702e-01  4.39425064e-01  9.05137288e-01  1.16533463e+00\n",
            "  -1.14303148e+00  6.29175380e-01  7.96755028e-01  5.81544212e-01\n",
            "   3.78979039e-01  2.44603111e+00]\n",
            " [ 1.00000000e+00  4.43932021e-01  2.00763875e-01 -6.03747319e-02\n",
            "   1.51660791e-01 -7.54201726e-01 -1.43426677e+00 -1.53533585e+00\n",
            "   6.56394314e-02 -1.66605483e+00  2.34414051e-01 -1.12951789e+00\n",
            "  -2.00122516e-01  1.05428211e-01]\n",
            " [ 1.00000000e+00  1.25920690e+00 -5.89179693e-01 -5.72122459e-01\n",
            "  -1.04947918e+00 -2.62708342e-01  5.68647662e-01  3.01923251e-01\n",
            "  -8.20719236e-01  6.81737904e-01 -1.54898934e-01  3.62177276e-01\n",
            "   1.38181344e+00  9.17474115e-01]\n",
            " [ 1.00000000e+00  7.65100912e-01  2.34617879e+00 -6.03747319e-02\n",
            "   1.51660791e-01 -5.43561704e-01 -4.72867844e-01 -1.23414583e+00\n",
            "   8.71420038e-01 -1.00026286e+00 -2.84669929e-01 -2.08176757e-01\n",
            "  -7.93348498e-01 -6.27005349e-01]\n",
            " [ 1.00000000e+00  3.45110824e-01 -6.25086219e-01  1.73074231e+00\n",
            "  -1.19962167e+00  7.20278427e-01  4.88531085e-01  6.53311604e-01\n",
            "  -1.76094750e-01 -4.04554257e-01 -1.98155932e-01  5.81544212e-01\n",
            "   2.37734757e-01  4.23877585e-01]\n",
            " [ 1.00000000e+00 -6.80159097e-01  6.22665554e-01  9.99674132e-01\n",
            "   2.25365574e+00 -1.92495001e-01 -6.33100999e-01 -1.45501851e+00\n",
            "   2.16066901e+00 -7.90012766e-01  1.05629702e+00 -1.26113805e+00\n",
            "  -1.24533020e+00  4.23877585e-01]\n",
            " [ 1.00000000e+00  6.09998823e-02  1.36772596e+00 -1.70034959e-01\n",
            "   9.02373272e-01 -1.03505509e+00 -1.03368389e+00 -4.41012124e-01\n",
            "   1.99951289e+00  5.09876168e-02 -1.11641936e-01 -5.15290467e-01\n",
            "  -8.49846211e-01 -7.38462629e-01]\n",
            " [ 1.00000000e+00  2.95700226e-01  2.27693770e-01  1.84040254e+00\n",
            "   4.51945783e-01  1.28198515e+00  8.08997395e-01  6.63351271e-01\n",
            "   2.26795553e-01  4.01404443e-01 -3.19275528e-01  3.62177276e-01\n",
            "   4.49601179e-01 -3.78740070e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación realizamos las predicciones según lo que obtuvimos en el entrenamiento."
      ],
      "metadata": {
        "id": "ySqT1h8pAYyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = sigmoid_function(X_test.dot(theta))"
      ],
      "metadata": {
        "id": "y93qFMfl4f7X"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación evaluamos la eficacia de nuestro modelo comparando nuestras predicciones contra los valores de y_test"
      ],
      "metadata": {
        "id": "jx5N72KMAhcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(x): # Convertir el valor continuo en uno binario, 0 o 1\n",
        "  if x > 0.5:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def evaluate(y_test, y_pred):\n",
        "  correct = 0\n",
        "  for i in range(len(y_test)):\n",
        "    print('Actual value:\\t\\t', y_test[i])\n",
        "    print('Predicted value:\\t', classify(y_pred[i]))\n",
        "    print('Full predicted value: \\t', y_pred[i])\n",
        "    print('Correct?:\\t\\t', (y_test[i] == classify(y_pred[i])))\n",
        "    print()\n",
        "\n",
        "    if y_test[i] == classify(y_pred[i]): correct += 1\n",
        "\n",
        "  print('---')\n",
        "  print('Correct: ', correct, '/', len(y_test))\n",
        "  print('Accuracy:', (correct/len(y_test))*100, '%')\n",
        "\n",
        "evaluate(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_vxoKs35ekt",
        "outputId": "241cdb00-db37-448d-acde-f63715511b8e"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.49364655]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.29780825]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.6157181]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.68944528]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.02620235]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.80135226]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.28020133]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.94325695]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.18930678]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99830432]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99721822]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [8.29582816e-06]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.83272861]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.33874675]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.44180694]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.01411351]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99800967]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99557474]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.9979869]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.00434995]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.32129989]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.02582804]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99045242]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.1126289]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.98107491]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99048426]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.98855919]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.92061658]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.97996605]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.09111647]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.9682535]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 0\n",
            "Full predicted value: \t [0.2507083]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.98838438]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 1\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.50504164]\n",
            "Correct?:\t\t True\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.59228079]\n",
            "Correct?:\t\t False\n",
            "\n",
            "Actual value:\t\t 0\n",
            "Predicted value:\t 1\n",
            "Full predicted value: \t [0.99790643]\n",
            "Correct?:\t\t False\n",
            "\n",
            "---\n",
            "Correct:  14 / 36\n",
            "Accuracy: 38.88888888888889 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al analizar los resultados, el modelo no fue nada bueno prediciendo. Al ejecutar el código muchas veces, el mejor accuracy que obtuve fue de 75%, sin embargo, el resto del tiempo obtengo valores por debajo del 50%. El entrenamiento del modelo está realizado de forma correcta. Quizás las fallas para predecir los resultados se deben al bajo nivel de entrenamiento del modelo, ya que solo cuenta con 142 registros en total. Al dividir estos datos en conjuntos de entrenamiento y prueba, solo quedan 108 registros para entrenar al modelo. Independientemente de la cantidad de epochs por las que entrenemos al modelo, si la cantidad de datos de entrenamiento no es suficientemente grande el modelo no logrará ser preciso."
      ],
      "metadata": {
        "id": "GUEOW8QNBHSq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SznZ7wj-CUiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}